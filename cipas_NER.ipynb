{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220206_cipas_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPHctw9/LsZjC71r8qnqN17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuYenTing/d4sg_cipas/blob/main/cipas_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CKIP實體辨識\n",
        "\n",
        "2022/02/06 蘇彥庭\n",
        "\n",
        "參考文件:\n",
        "[Github: ckiptagger](https://github.com/ckiplab/ckiptagger)"
      ],
      "metadata": {
        "id": "nEIxpJfFjvCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68T4_p0JiwJ3",
        "outputId": "9a2d5268-cacd-4b6b-c41d-58e2923a9033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 掛載雲端硬碟\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝套件\n",
        "!pip install -U ckiptagger\n",
        "\n",
        "# 下載模型\n",
        "from ckiptagger import data_utils\n",
        "data_utils.download_data_gdown(\"./\")"
      ],
      "metadata": {
        "id": "7WTUSQBejOP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea7bbb6-ee78-4c38-be0c-82c1aa8e3a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ckiptagger\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:06<00:00, 271MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入套件\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from ckiptagger import construct_dictionary, WS, POS, NER\n",
        "# from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "IYuHbFBzjD2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 讀取資料\n",
        "cipasWebContentData = pd.read_csv('/content/drive/MyDrive/[不當黨產]/Data/爬蟲資料/cipasWebContentData.csv')\n",
        "pdfData = pd.read_csv('/content/drive/MyDrive/[不當黨產]/Data/爬蟲資料/pdfData.csv')"
      ],
      "metadata": {
        "id": "nBvo7kNBjCPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 取出調查報告文章資料(PDF檔案內容)\n",
        "reportData = pdfData[(pdfData['source'] == 'news') & (pdfData['attachFileNames'].str.contains('調查報告'))]\n",
        "reportData = reportData[~(reportData['attachFileNames'].str.contains('PPT|簡報|投影片'))]\n",
        "reportData = reportData[['source', 'id', 'attachFileNames', 'content']].rename(columns={'attachFileNames': 'articleTitle', 'content': 'articleContent'})\n",
        "\n",
        "# 取出史料故事文章資料(網站內容)\n",
        "storiesData = cipasWebContentData[cipasWebContentData['source'] == 'stories'][['source', 'id', 'articleTitle', 'articleContent']]\n",
        "\n",
        "# 合併資料\n",
        "docData = pd.concat([reportData, storiesData]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Gi2wBaxNlKqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入模型\n",
        "ws = WS(\"./data\", disable_cuda=False)  # disable_cuda=False: use gpu\n",
        "pos = POS(\"./data\", disable_cuda=False)\n",
        "ner = NER(\"./data\", disable_cuda=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULj1BvNHKpXC",
        "outputId": "f8c2d36e-f8f2-4a64-bc05-f222da31127d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 讀取Google雲端sheet的自定義斷詞\n",
        "customWords = []\n",
        "\n",
        "sheet_urls = [\n",
        "              'https://docs.google.com/spreadsheets/d/1ayhAdNJ_Tc9-6ptEkCZSQe-GJ0obpZoAsHvXIUVFOK4/edit#gid=1370388179', \n",
        "              'https://docs.google.com/spreadsheets/d/1ayhAdNJ_Tc9-6ptEkCZSQe-GJ0obpZoAsHvXIUVFOK4/edit#gid=725229876',\n",
        "              ]\n",
        "\n",
        "for sheet_url in sheet_urls:\n",
        "    \n",
        "    sheet_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "    iCustomWords = pd.read_csv(sheet_url, header=None)\n",
        "    iCustomWords = iCustomWords.to_numpy().flatten()\n",
        "    iCustomWords = iCustomWords[~pd.isnull(iCustomWords)]\n",
        "    iCustomWords = list(set(iCustomWords))\n",
        "    customWords = customWords + iCustomWords\n",
        "\n",
        "customWords = list(set(customWords))\n",
        "\n",
        "# 建立自定義詞字典\n",
        "dictionary = {elem: 1 for elem in customWords}\n",
        "dictionary = construct_dictionary(dictionary)"
      ],
      "metadata": {
        "id": "pcH9gIJoOzse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 清洗資料函數\n",
        "def cleanText(text):\n",
        "    # 移除數字, 空格(含跳行), 英文句點, 英文逗點\n",
        "    # text = re.sub('\\d|\\s|\\.|\\,', '', text)\n",
        "\n",
        "    # 移除空格(含跳行)\n",
        "    text = re.sub('\\s', '', text)\n",
        "    return text\n",
        "\n",
        "# 建立儲存表\n",
        "personDict = dict()  # 儲存人名(PERSON)實體辨識結果\n",
        "orgDict = dict()  # 儲存組織(ORG)實體辨識結果\n",
        "wordSentenceList = []  # 儲存各句斷詞後結果\n",
        "\n",
        "# 迴圈文本資料\n",
        "# i = 0\n",
        "for i in tqdm(range(len(docData))):\n",
        "\n",
        "    # 讀取文本並清洗資料\n",
        "    doc = docData['articleContent'][i]\n",
        "    doc = cleanText(doc)\n",
        "\n",
        "    # 以句號作為句子分隔\n",
        "    doc = doc.split('。')\n",
        "\n",
        "    # 執行CKIP斷詞\n",
        "    word_sentence_list = ws(\n",
        "        doc,\n",
        "        sentence_segmentation=True,  # To consider delimiters\n",
        "        segment_delimiter_set={\",\", \"。\", \":\", \"?\", \"!\", \";\"},  # This is the defualt set of delimiters\n",
        "        coerce_dictionary=dictionary,  # words in this dictionary are forced\n",
        "        )\n",
        "\n",
        "    # 儲存斷詞結果\n",
        "    for word_sentence in word_sentence_list:\n",
        "        wordSentenceList.append(word_sentence)\n",
        "        \n",
        "    # 執行CKIP詞性標註\n",
        "    pos_sentence_list = pos(word_sentence_list)\n",
        "\n",
        "    # 執行CKIP實體辨識\n",
        "    entity_sentence_list = ner(word_sentence_list, pos_sentence_list)\n",
        "\n",
        "    # 依CKIP實體辨識取出人名(PERSON)與組織(ORG) 並計算該詞出現的次數\n",
        "    for res in entity_sentence_list:\n",
        "        for elem in res:\n",
        "            if elem[2] == 'PERSON':\n",
        "                if elem[3] not in personDict:  # 若第一次出現的詞則加入字典並初始化次數為1\n",
        "                    personDict[elem[3]] = 1\n",
        "                else:\n",
        "                    personDict[elem[3]] += 1  # 先前已出現過該詞 計數再加1次\n",
        "            elif elem[2] == 'ORG':\n",
        "                if elem[3] not in orgDict:\n",
        "                    orgDict[elem[3]] = 1\n",
        "                else:\n",
        "                    orgDict[elem[3]] += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPQ04GEQ_ukm",
        "outputId": "1b43be08-116d-4a91-b550-f2f44420a1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 354/354 [12:14<00:00,  2.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 將實體辨識結果轉為資料表\n",
        "personEntityDf = pd.DataFrame(data={'person': personDict.keys(), 'counts': personDict.values()})\n",
        "personEntityDf = personEntityDf.sort_values(['counts'], ascending=False)  # 案詞頻排序\n",
        "personEntityDf = personEntityDf[personEntityDf['person'].str.len() > 1]  # 過濾掉只有1個字的詞\n",
        "personEntityDf = personEntityDf[~personEntityDf['person'].str.contains('○|〇|O|Ｏ')]  # 刪除具遮蔽之名稱\n",
        "personEntityDf = personEntityDf.reset_index(drop=True)\n",
        "\n",
        "orgEntityDf = pd.DataFrame(data={'org': orgDict.keys(), 'counts': orgDict.values()})\n",
        "orgEntityDf = orgEntityDf.sort_values(['counts'], ascending=False)\n",
        "orgEntityDf = orgEntityDf[orgEntityDf['org'].str.len() > 1]\n",
        "orgEntityDf = orgEntityDf.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FDM16xGl59PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 輸出結果\n",
        "personEntityDf.to_csv('personEntityDf.csv', index=False, encoding='utf-8-sig')\n",
        "orgEntityDf.to_csv('orgEntityDf.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "SNYnCnBoq5t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 儲存斷詞後結果\n",
        "# with open('/content/drive/MyDrive/d4sg/doc_ws.pickle', 'wb') as f:\n",
        "#     pickle.dump([wordSentenceList, personEntityDf, orgEntityDf], f)\n",
        "\n",
        "# 讀取斷詞後結果\n",
        "with open('/content/drive/MyDrive/d4sg/doc_ws.pickle', 'rb') as f:\n",
        "    wordSentenceList, personEntityDf, orgEntityDf = pickle.load(f)"
      ],
      "metadata": {
        "id": "PbmA4nMJCsNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 處理姓名及職稱問題\n",
        "forPersonJobEntityDf = personEntityDf[personEntityDf['person'].str.len() >= 1]\n",
        "\n",
        "# 處理姓氏\n",
        "def lastName(person):\n",
        "    if len(person) >= 4:  # 名字4個字以上 則姓氏取前2個字\n",
        "        return person[0:2]\n",
        "    else:\n",
        "        return person[0]\n",
        "\n",
        "# 處理名字\n",
        "def firstName(person):\n",
        "    if len(person) >= 4:  # 名字4個字以上 則名字取第二個字以後\n",
        "        return person[2:]\n",
        "    else:\n",
        "        return person[1:]\n",
        "\n",
        "forPersonJobEntityDf['lastName'] = personEntityDf['person'].apply(lastName)\n",
        "forPersonJobEntityDf['firstName'] = personEntityDf['person'].apply(firstName)"
      ],
      "metadata": {
        "id": "goZS1Z7HvJ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立姓氏+職稱+名字儲存表\n",
        "matchPersonJob = list()\n",
        "\n",
        "# sentence = wordSentenceList[8284]\n",
        "# 迴圈各句比對是否有姓氏+職稱+名字儲存表\n",
        "for i in tqdm(range(len(wordSentenceList))):\n",
        "\n",
        "    sentence = wordSentenceList[i]\n",
        "\n",
        "    # 判斷該詞是否有符合的名字 若有符合則再對應姓氏 若皆符合名字及姓氏 則儲存姓氏+職稱+名字\n",
        "    for idx, value in enumerate(sentence):\n",
        "        if idx > 1:\n",
        "            matchLastName = forPersonJobEntityDf[forPersonJobEntityDf['firstName'] == value]['lastName'].tolist()\n",
        "            if sentence[idx-2] in matchLastName:\n",
        "                matchPersonJob.append(sentence[(idx-2):(idx+1)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXsajc4m8PAr",
        "outputId": "bef06e91-e750-4b18-9f51-6f53266ed423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20365/20365 [06:42<00:00, 50.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 將姓氏+職稱+名字整理成資料表格\n",
        "personJobEntityDf = pd.DataFrame(matchPersonJob, columns=['lastName', 'jobTitle', 'firstName'])\n",
        "personJobEntityDf = personJobEntityDf.drop_duplicates()\n",
        "personJobEntityDf = personJobEntityDf.sort_values('lastName', ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "7kGpBAC1UUyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 輸出結果\n",
        "personJobEntityDf.to_csv('personJobEntityDf.csv', index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "mg1xRqRPUUuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M9MXuaRlb5kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}