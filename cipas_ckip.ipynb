{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220213_cipas_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMd9SOtg9ttuIPyQSLQDx1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuYenTing/d4sg_cipas/blob/main/cipas_ckip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CKIP實體辨識\n",
        "\n",
        "2022/02/06 蘇彥庭\n",
        "\n",
        "參考文件:\n",
        "* [Github: ckiptagger](https://github.com/ckiplab/ckiptagger)\n",
        "* [基於詞性組合規則結合維基百科\n",
        "進行中文命名實體辨識與消歧義](https://www.lac.org.tw/sites/default/files/field_files/publish/attach241_1.pdf)"
      ],
      "metadata": {
        "id": "nEIxpJfFjvCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68T4_p0JiwJ3",
        "outputId": "8babab71-cbb9-4df9-e587-97ebd7361fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 掛載雲端硬碟\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 安裝套件\n",
        "!pip install -U ckiptagger"
      ],
      "metadata": {
        "id": "7WTUSQBejOP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13546501-99fb-4712-bc0c-fd1741f5f79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ckiptagger\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 下載模型\n",
        "!pip install --upgrade --no-cache-dir gdown  # 需升級gdown套件避免下載模型時報錯\n",
        "from ckiptagger import data_utils\n",
        "data_utils.download_data_gdown(\"/content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynn1ADyEUMUk",
        "outputId": "1cbff0ba-4695-49ff-aaaf-fb6cca05ae11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.1)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.3.0.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.3.0-py3-none-any.whl size=14412 sha256=6addab467bf8f15241c7b40177ff67c78dad0bb837b0908f1f9372e6a17f7e2e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d8diqlnh/wheels/fd/ce/f8/389eafb78bce55ea78740dfcafc3c9da6f5e70d25c0377610d\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.2.1\n",
            "    Uninstalling gdown-4.2.1:\n",
            "      Successfully uninstalled gdown-4.2.1\n",
            "Successfully installed gdown-4.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:10<00:00, 177MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入套件\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from ckiptagger import construct_dictionary, WS, POS, NER\n",
        "# from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "IYuHbFBzjD2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 讀取資料\n",
        "cipasWebContentData = pd.read_csv('/content/drive/MyDrive/[不當黨產]/Data/爬蟲資料/cipasWebContentData.csv')\n",
        "pdfData = pd.read_csv('/content/drive/MyDrive/[不當黨產]/Data/爬蟲資料/pdfData.csv')"
      ],
      "metadata": {
        "id": "nBvo7kNBjCPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 取出調查報告文章資料(PDF檔案內容)\n",
        "# reportData = pdfData[(pdfData['source'] == 'news') & (pdfData['attachFileNames'].str.contains('調查報告'))]\n",
        "# reportData = reportData[~(reportData['attachFileNames'].str.contains('PPT|簡報|投影片'))]\n",
        "# reportData = reportData[['source', 'id', 'attachFileNames', 'content']].rename(columns={'attachFileNames': 'articleTitle', 'content': 'articleContent'})\n",
        "\n",
        "# 取出史料故事文章資料(網站內容)\n",
        "storiesData = cipasWebContentData[cipasWebContentData['source'] == 'stories'][['source', 'id', 'articleTitle', 'articleContent']]\n",
        "\n",
        "# 合併資料\n",
        "# docData = pd.concat([reportData, storiesData]).reset_index(drop=True)\n",
        "docData = storiesData.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Gi2wBaxNlKqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 載入模型\n",
        "wsModel = WS(\"./data\", disable_cuda=False)  # disable_cuda=False: use gpu\n",
        "posModel = POS(\"./data\", disable_cuda=False)\n",
        "nerModel = NER(\"./data\", disable_cuda=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULj1BvNHKpXC",
        "outputId": "b0913992-78b8-43cf-b8b1-cbaae66bcaca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # 讀取Google雲端sheet的自定義斷詞\n",
        "customWords = []\n",
        "\n",
        "sheetDict = {\n",
        "    '不當黨產相關名詞詞庫_黨產會斷詞庫': 'https://docs.google.com/spreadsheets/d/1ayhAdNJ_Tc9-6ptEkCZSQe-GJ0obpZoAsHvXIUVFOK4/edit#gid=1370388179',\n",
        "    '不當黨產相關名詞詞庫_資料英雄斷詞': 'https://docs.google.com/spreadsheets/d/1ayhAdNJ_Tc9-6ptEkCZSQe-GJ0obpZoAsHvXIUVFOK4/edit#gid=725229876',\n",
        "    '不當黨產相關名詞詞庫_直式範本': 'https://docs.google.com/spreadsheets/d/1ayhAdNJ_Tc9-6ptEkCZSQe-GJ0obpZoAsHvXIUVFOK4/edit#gid=1583708380',\n",
        "}\n",
        "\n",
        "for sheetName in sheetDict:\n",
        "\n",
        "    sheetUrl = sheetDict[sheetName]\n",
        "    sheetUrl = sheetUrl.replace('/edit#gid=', '/export?format=csv&gid=')\n",
        "\n",
        "    if sheetName == '不當黨產相關名詞詞庫_黨產會斷詞庫':\n",
        "        iCustomWords = pd.read_csv(sheetUrl, header=None)\n",
        "        iCustomWords = iCustomWords.iloc[:, 0:2]\n",
        "\n",
        "    elif sheetName == '不當黨產相關名詞詞庫_資料英雄斷詞':\n",
        "        iCustomWords = pd.read_csv(sheetUrl, header=None)\n",
        "        iCustomWords = iCustomWords.iloc[:, 0:1]\n",
        "\n",
        "    elif sheetName == '不當黨產相關名詞詞庫_直式範本':\n",
        "        iCustomWords = pd.read_csv(sheetUrl)\n",
        "\n",
        "    iCustomWords = iCustomWords.to_numpy().flatten()\n",
        "    iCustomWords = iCustomWords[~pd.isnull(iCustomWords)]\n",
        "    iCustomWords = list(set(iCustomWords))\n",
        "    customWords = customWords + iCustomWords\n",
        "\n",
        "# 刪除重複的詞\n",
        "customWords = list(set(customWords))\n",
        "\n",
        "# 建立自定義詞字典\n",
        "dictionary = {elem: 1 for elem in customWords}\n",
        "dictionary = construct_dictionary(dictionary)"
      ],
      "metadata": {
        "id": "BX8buNiu45-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 清洗資料函數\n",
        "def cleanText(text):\n",
        "    \n",
        "    # 移除數字, 空格(含跳行), 英文句點, 英文逗點\n",
        "    # text = re.sub('\\d|\\s|\\.|\\,', '', text)\n",
        "\n",
        "    # 移除空格(含跳行)\n",
        "    text = re.sub('\\s', '', text)\n",
        "\n",
        "    # 移除 資料來源 / 本文參考 / 全文參考 / 引自 (含)以後的字元\n",
        "    matchSite = re.search('資料來源|本文參考|全文參考|引自', text)\n",
        "    if matchSite:\n",
        "        text = text[:matchSite.span()[0]]\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Z3PpFCumqXgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立儲存表\n",
        "wordSentenceList = []  # 儲存各句斷詞後結果\n",
        "entityList = []  # 儲存實體辨識結果\n",
        "posList = []  # 儲存詞性結果\n",
        "personJobList = []  # 儲存姓+職稱+名(利用詞性Nb+Na+Nb組合)\n",
        "\n",
        "# 篩選詞性清單\n",
        "targetPos = ['Na', 'Nb', 'Nc']\n",
        "# 篩選實體清單\n",
        "targetEntity = ['ORG', 'PERSON', 'EVENT', 'FAC', 'GPE', 'LOC', 'WORK_OF_ART']\n",
        "\n",
        "# 迴圈文本資料\n",
        "# iDoc = 200\n",
        "for iDoc in tqdm(range(len(docData))):\n",
        "\n",
        "    # 讀取文本並清洗資料\n",
        "    doc = docData['articleContent'][iDoc]\n",
        "    doc = cleanText(doc)\n",
        "\n",
        "    # 以句號作為句子分隔\n",
        "    doc = doc.split('。')\n",
        "\n",
        "    # 執行CKIP斷詞\n",
        "    wsModelResult = wsModel(\n",
        "        doc,\n",
        "        sentence_segmentation=True,  # To consider delimiters\n",
        "        segment_delimiter_set={\",\", \"。\", \":\", \"?\", \"!\", \";\"},  # This is the defualt set of delimiters\n",
        "        coerce_dictionary=dictionary,  # words in this dictionary are forced\n",
        "        )\n",
        "    # 執行CKIP詞性標註\n",
        "    posModelResult = posModel(wsModelResult)\n",
        "    # 執行CKIP實體辨識\n",
        "    entityModelResult = nerModel(wsModelResult, posModelResult)\n",
        "\n",
        "    # 儲存斷詞結果\n",
        "    for word_sentence in wsModelResult:\n",
        "        wordSentenceList.append(word_sentence)\n",
        "\n",
        "    # 儲存詞性結果\n",
        "    for i in range(len(wsModelResult)):\n",
        "\n",
        "        wordSetence = wsModelResult[i]\n",
        "        posSentence = posModelResult[i]\n",
        "        assert len(wordSetence) == len(posSentence)\n",
        "\n",
        "        # 整理目標詞性\n",
        "        for word, pos in zip(wordSetence, posSentence):\n",
        "            if pos in targetPos:\n",
        "                if len(word) > 1:  # 詞至少要有一個字(單字不成詞)\n",
        "                    posList.append([word, pos])\n",
        "\n",
        "        # 整理姓+職稱+名(利用詞性Nb+Na+Nb組合)\n",
        "        for i in range(len(posSentence)):\n",
        "            if posSentence[i:(i+3)] == ['Nb', 'Na', 'Nb']:\n",
        "                if (len(wordSetence[i]) <= 2) and (len(wordSetence[i+2]) <= 2):  # 姓和名只能2個字以下\n",
        "                    personJobList.append(wordSetence[i:(i+3)])\n",
        "                    \n",
        "    # 儲存實體辨識結果\n",
        "    for res in entityModelResult:\n",
        "        for elem in res:\n",
        "            if elem[2] in targetEntity:  # 判斷是否在想要關注的實體標示類別內\n",
        "                if len(elem[3]) > 1:  # 詞至少要有一個字(單字不成詞)\n",
        "                    entityList.append([elem[3], elem[2]])\n",
        "\n",
        "    # # 印出CKIP結果\n",
        "    # def print_word_pos_sentence(word_sentence, pos_sentence):\n",
        "    #     assert len(word_sentence) == len(pos_sentence)\n",
        "    #     for word, pos in zip(word_sentence, pos_sentence):\n",
        "    #         print(f\"{word}({pos})\", end=\"\\u3000\")\n",
        "    #     print()\n",
        "    #     return\n",
        "\n",
        "    # for i, sentence in enumerate(wsModelResult):\n",
        "    #     print()\n",
        "    #     print(f\"'{sentence}'\")\n",
        "    #     print_word_pos_sentence(wsModelResult[i],  posModelResult[i])\n",
        "    #     for entity in sorted(entityModelResult[i]):\n",
        "    #         print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i2YXO5fV6J8",
        "outputId": "349708ff-5cfa-4625-a499-7cdd6f838352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 298/298 [03:24<00:00,  1.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 整理輸出結果\n",
        "entityDf = pd.DataFrame(entityList, columns=['word', 'type'])\n",
        "posDf = pd.DataFrame(posList, columns=['word', 'type'])\n",
        "personJobDf = pd.DataFrame(personJobList, columns=['lastName', 'jobTitle', 'firstName'])\n",
        "personJobDf['person'] = personJobDf['lastName'] + personJobDf['jobTitle'] + personJobDf['firstName']\n",
        "personJobDf['word'] = personJobDf['lastName'] + personJobDf['firstName']"
      ],
      "metadata": {
        "id": "Nk1qpYiVV8VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 計算詞頻數量\n",
        "entityDf = entityDf.groupby(['word', 'type']).size().reset_index(name='counts').sort_values(['counts'], ascending=False).reset_index(drop=True)\n",
        "posDf = posDf.groupby(['word', 'type']).size().reset_index(name='counts').sort_values(['counts'], ascending=False).reset_index(drop=True)\n",
        "personJobDf = personJobDf.groupby(['lastName', 'jobTitle', 'firstName', 'person', 'word']).size().reset_index(name='counts').sort_values(['counts'], ascending=False).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "M_w40UbQV8TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增欄位判斷是否已在斷詞辭典內\n",
        "entityDf['inDict'] = entityDf['word'].isin(customWords).apply(lambda x: '1' if x else '')\n",
        "posDf['inDict'] = posDf['word'].isin(customWords).apply(lambda x: '1' if x else '')\n",
        "personJobDf['inDict'] = personJobDf['person'].isin(customWords).apply(lambda x: '1' if x else '')"
      ],
      "metadata": {
        "id": "_6WwswsahKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 輸出結果\n",
        "entityDf.to_csv('entityDf.csv', index=False, encoding='utf-8-sig')\n",
        "posDf.to_csv('posDf.csv', index=False, encoding='utf-8-sig') \n",
        "personJobDf.to_csv('personJobDf.csv', index=False, encoding='utf-8-sig') "
      ],
      "metadata": {
        "id": "X7_qpUG0V8RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認那些字典自定義詞沒有被捕捉到\n",
        "catchWordLists = entityDf['word'][entityDf['inDict'] == '1'].tolist() + \\\n",
        "                 posDf['word'][posDf['inDict'] == '1'].tolist() + \\\n",
        "                 personJobDf['person'][personJobDf['inDict'] == '1'].tolist()\n",
        "notCatchWordLists = [elem for elem in customWords if elem not in catchWordLists]"
      ],
      "metadata": {
        "id": "J-__U9BTkFTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T0U15F2h0Q0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 確認實體辨識結果是否皆在詞性標註結果內\n",
        "orgAndPerson = entityDf[entityDf['type'].isin(['ORG', 'PERSON'])]['word'].tolist()\n",
        "orgAndPersonNotInPos = [elem for elem in orgAndPerson if elem not in posDf[posDf['type'].isin(['Na', 'Nb', 'Nc'])]['word'].tolist()]"
      ],
      "metadata": {
        "id": "NPzJn_lqnOpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將實體辨識結果併入詞性內\n",
        "posDf['ORG'] = posDf['word'].isin(entityDf[entityDf['type'] == 'ORG']['word']).apply(lambda x: 'ORG' if x else '')\n",
        "posDf['PERSON'] = posDf['word'].isin(entityDf[entityDf['type'] == 'PERSON']['word']).apply(lambda x: 'PERSON' if x else '')"
      ],
      "metadata": {
        "id": "mshbVtGDquC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posDf.to_csv('posDf.csv', index=False, encoding='utf-8-sig') "
      ],
      "metadata": {
        "id": "n06zzmn3sUYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LOaOysxSzrQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}